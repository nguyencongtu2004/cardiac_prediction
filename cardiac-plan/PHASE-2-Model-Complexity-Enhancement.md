
## üéØ PHASE 2: MODEL COMPLEXITY ENHANCEMENT (Tu·∫ßn 1-2)

### M·ª•c ti√™u Phase 2
- N√¢ng c·∫•p model t·ª´ basic Random Forest l√™n ensemble ph·ª©c t·∫°p
- Implement hyperparameter tuning v·ªõi CrossValidator
- TƒÉng AUC-ROC t·ª´ ~0.75 l√™n >0.85

---

### B∆Ø·ªöC 2.1: Advanced Feature Engineering Pipeline (4 gi·ªù)

#### Nhi·ªám v·ª• ch√≠nh:

1. **Thi·∫øt k·∫ø Pipeline n√¢ng cao**
   - **Stage 1**: StringIndexer cho categorical features (n·∫øu c√≥)
   - **Stage 2**: OneHotEncoder cho encoded categories
   - **Stage 3**: VectorAssembler cho continuous features
   - **Stage 4**: StandardScaler ƒë·ªÉ normalize
   - **Stage 5**: PolynomialExpansion (degree=2) cho interactions
   - **Stage 6**: ChiSqSelector ƒë·ªÉ feature selection
   - **Stage 7**: Final VectorAssembler

2. **Document pipeline flow**
   - V·∫Ω diagram cho pipeline stages
   - Explain √Ω nghƒ©a t·ª´ng transformation
   - Document input/output c·ªßa m·ªói stage

3. **Implement trong Spark**
   - Code t·ª´ng stage ri√™ng bi·ªát
   - Test output c·ªßa t·ª´ng stage
   - Chain stages l·∫°i th√†nh pipeline

#### Output:
- Pipeline architecture document
- Feature engineering pipeline implementation
- Unit tests cho t·ª´ng stage

---

### B∆Ø·ªöC 2.2: Multiple Classifier Implementation (5 gi·ªù)

#### Nhi·ªám v·ª• ch√≠nh:

1. **Implement 3 classifiers**
   - **Random Forest**: Baseline, robust
   - **Gradient Boosted Trees**: Potentially higher accuracy
   - **Logistic Regression**: Interpretable, fast

2. **Configure m·ªói classifier**
   - Random Forest:
     - numTrees: 100-300
     - maxDepth: 10-20
     - minInstancesPerNode: 1-10
   - GBT:
     - maxIter: 50-100
     - maxDepth: 5-10
   - Logistic Regression:
     - maxIter: 100
     - regParam: 0.01-0.1
     - elasticNetParam: 0-1

3. **Train v√† evaluate ri√™ng bi·ªát**
   - Train m·ªói model tr√™n c√πng training set
   - Evaluate tr√™n validation set
   - So s√°nh metrics: AUC, F1, Precision, Recall
   - Document strengths/weaknesses c·ªßa m·ªói model

#### Output:
- 3 trained models v·ªõi metrics
- Comparison report
- Recommendation model n√†o t·ªët nh·∫•t

---

### B∆Ø·ªöC 2.3: Hyperparameter Tuning v·ªõi CrossValidator (6 gi·ªù)

#### Nhi·ªám v·ª• ch√≠nh:

1. **Thi·∫øt k·∫ø param grid**
   - X√°c ƒë·ªãnh hyperparameters quan tr·ªçng nh·∫•t
   - Define ranges h·ª£p l√Ω cho m·ªói param
   - Balance grid size vs computation time
   - **V√≠ d·ª• cho Random Forest**:
     - numTrees: [100, 200, 300]
     - maxDepth: [10, 15, 20]
     - minInstancesPerNode: [1, 5, 10]
     - maxBins: [32, 64]
     - ‚Üí Total: 3√ó3√ó3√ó2 = 54 combinations

2. **Setup CrossValidator**
   - Ch·ªçn evaluator (BinaryClassificationEvaluator v·ªõi AUC metric)
   - numFolds: 5 (trade-off accuracy vs time)
   - parallelism: 4 (t·∫≠n d·ª•ng multi-core)
   - Seed cho reproducibility

3. **Run hyperparameter tuning**
   - Execute CrossValidator.fit()
   - Monitor progress v√† resource usage
   - **L∆∞u √Ω**: Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t 2-4 gi·ªù

4. **Analyze results**
   - Extract best model
   - Compare v·ªõi baseline (model kh√¥ng tune)
   - Document best hyperparameters
   - Visualize parameter importance

#### Output:
- Best model t·ª´ CrossValidator
- Hyperparameter tuning report
- Comparison chart: tuned vs baseline

---

### B∆Ø·ªöC 2.4: Ensemble Strategy (4 gi·ªù)

#### Nhi·ªám v·ª• ch√≠nh:

1. **Evaluate ensemble options**
   - **Option 1**: Voting Classifier (majority vote)
   - **Option 2**: Weighted Voting (based on validation AUC)
   - **Option 3**: Stacking (train meta-model)

2. **Implement ensemble logic**
   - **L∆∞u √Ω**: Spark MLlib kh√¥ng c√≥ built-in VotingClassifier
   - C·∫ßn custom implementation:
     - Train multiple models ri√™ng bi·ªát
     - Combine predictions (average probabilities ho·∫∑c vote)
     - Evaluate ensemble performance

3. **Compare ensemble vs single model**
   - Test tr√™n validation set
   - Check if ensemble improves AUC
   - Consider computational overhead

4. **Decision: Use ensemble or best single model?**
   - Trade-off: accuracy gain vs complexity
   - Production deployment considerations

#### Output:
- Ensemble implementation (n·∫øu ch·ªçn ensemble)
- Performance comparison report
- Final model selection decision

---

### B∆Ø·ªöC 2.5: Model Evaluation v√† Validation (3 gi·ªù)

#### Nhi·ªám v·ª• ch√≠nh:

1. **Comprehensive evaluation tr√™n test set**
   - AUC-ROC
   - Precision, Recall, F1
   - Confusion Matrix
   - PR Curve (Precision-Recall)

2. **Generate evaluation artifacts**
   - Confusion matrix heatmap
   - ROC curve plot
   - Feature importance chart (t·ª´ RF/GBT)
   - Prediction distribution histogram

3. **Compare v·ªõi baseline model**
   - Old model (basic RF 100 trees)
   - New model (tuned ensemble)
   - Quantify improvement

4. **Medical validation**
   - Check false positive rate (kh√¥ng mu·ªën qu√° nhi·ªÅu false alarms)
   - Check false negative rate (critical - miss high-risk patients)
   - Discuss trade-offs v·ªõi domain experts

#### Output:
- Comprehensive evaluation report
- Visualization plots
- Comparison table: baseline vs new model
- Recommendation: Deploy new model?

---

### B∆Ø·ªöC 2.6: Update Model Training DAG (2 gi·ªù)

#### Nhi·ªám v·ª• ch√≠nh:

1. **Modify `cardiac_model_retraining_dag.py`**
   - Update task ƒë·ªÉ g·ªçi script training m·ªõi
   - Add tasks cho hyperparameter tuning
   - Add tasks cho ensemble training (n·∫øu d√πng)

2. **Add model versioning**
   - L∆∞u model v·ªõi version number tƒÉng d·∫ßn
   - Metadata file cho m·ªói version (hyperparams, metrics, date)

3. **Update model promotion logic**
   - Compare new model v·ªõi current production model
   - Auto-promote n·∫øu AUC improvement > threshold (vd: +0.02)
   - Rollback mechanism n·∫øu new model underperform

#### Output:
- Updated DAG file
- Model versioning strategy document
- Test DAG execution th√†nh c√¥ng
