## ğŸ¯ PHASE 1: DATASET UPGRADE (Tuáº§n 1)

### Má»¥c tiÃªu Phase 1
- Thay tháº¿ dá»¯ liá»‡u synthetic báº±ng dá»¯ liá»‡u y táº¿ thá»±c táº¿
- TÄƒng sá»‘ lÆ°á»£ng features tá»« 11 lÃªn 18-20
- Äáº£m báº£o data quality vÃ  medical relevance

---

### BÆ¯á»šC 1.1: NghiÃªn Cá»©u vÃ  Lá»±a Chá»n Dataset (4 giá»)

#### Nhiá»‡m vá»¥ chÃ­nh:
1. **Äá»c tÃ i liá»‡u 3 dataset Ä‘Æ°á»£c Ä‘á» xuáº¥t**
   - MIMIC-III
   - Heart Failure Clinical Records (Kaggle)
   - UCI Heart Disease

2. **So sÃ¡nh cÃ¡c tiÃªu chÃ­:**
   - Sá»‘ lÆ°á»£ng bá»‡nh nhÃ¢n
   - Sá»‘ lÆ°á»£ng features
   - TÃ­nh kháº£ dá»¥ng (licensing, download speed)
   - Äá»™ phá»©c táº¡p integration
   - Medical relevance

3. **Quyáº¿t Ä‘á»‹nh lá»±a chá»n:**
   - **Khuyáº¿n nghá»‹**: Heart Failure Clinical Records
   - **LÃ½ do**: 
     - Download nhanh (khÃ´ng cáº§n approval)
     - Äá»§ features cho medical prediction
     - Dá»… integrate vÃ o pipeline hiá»‡n cÃ³
     - 299 patients Ä‘á»§ cho proof-of-concept

#### Output:
- Document so sÃ¡nh 3 datasets (markdown file)
- Quyáº¿t Ä‘á»‹nh chÃ­nh thá»©c dataset sá»­ dá»¥ng
- URL download vÃ  credentials setup

---

### BÆ¯á»šC 1.2: Download vÃ  Kháº£o SÃ¡t Dá»¯ Liá»‡u (2 giá»)

#### Nhiá»‡m vá»¥ chÃ­nh:

1. **Setup Kaggle API**
   - Táº¡o API credentials tá»« Kaggle account
   - Configure kaggle.json file
   - Test connection

2. **Download dataset**
   - Download Heart Failure Clinical Records
   - Unzip vÃ o thÆ° má»¥c `data/`
   - Verify file integrity

3. **Kháº£o sÃ¡t cáº¥u trÃºc dá»¯ liá»‡u**
   - Xem 20 rows Ä‘áº§u tiÃªn
   - Check data types
   - XÃ¡c Ä‘á»‹nh missing values
   - PhÃ¢n tÃ­ch class distribution
   - Hiá»ƒu Ã½ nghÄ©a y há»c cá»§a tá»«ng feature

#### Output:
- Dataset Ä‘Ã£ download trong `data/heart_failure_clinical_records_dataset.csv`
- Document kháº£o sÃ¡t dá»¯ liá»‡u (sá»‘ records, features, distribution)

---

### BÆ¯á»šC 1.3: Thiáº¿t Káº¿ Feature Engineering Plan (3 giá»)

#### Nhiá»‡m vá»¥ chÃ­nh:

1. **Mapping features tá»« dataset má»›i sang schema hiá»‡n táº¡i**
   - XÃ¡c Ä‘á»‹nh features nÃ o giá»¯ nguyÃªn
   - XÃ¡c Ä‘á»‹nh features nÃ o cáº§n transform
   - XÃ¡c Ä‘á»‹nh features nÃ o bá»

2. **Thiáº¿t káº¿ derived features (5-7 features má»›i)**
   - **Age groups**: PhÃ¢n loáº¡i Ä‘á»™ tuá»•i (<50, 50-65, 65-75, >75)
   - **Kidney risk**: Indicator dá»±a trÃªn serum_creatinine
   - **Critical ejection**: Flag khi ejection_fraction < 30%
   - **Combined risk score**: Tá»•ng há»£p cÃ¡c yáº¿u tá»‘ nguy cÆ¡
   - **Feature interactions**: ejection_fraction Ã— serum_creatinine

3. **Thiáº¿t káº¿ schema má»›i**
   - 13 features gá»‘c tá»« dataset
   - 5-7 derived features
   - Total: 18-20 features
   - Document Ã½ nghÄ©a y há»c cá»§a má»—i feature

#### Output:
- Feature engineering specification document
- Mapping table: old schema â†’ new schema
- Medical rationale cho tá»«ng derived feature

---

### BÆ¯á»šC 1.4: Implement Data Preprocessing Script (4 giá»)

#### Nhiá»‡m vá»¥ chÃ­nh:

1. **Táº¡o script preprocessing má»›i**
   - File: `cardiac_data_prep_v2.py`
   - Load CSV dataset
   - Apply feature transformations
   - Create derived features

2. **Feature transformations cáº§n implement:**
   - **Bucketizer** cho age groups
   - **Conditional logic** cho kidney_risk
   - **Threshold-based** features
   - **Mathematical interactions**

3. **Train/Valid/Test split**
   - 70% training
   - 15% validation
   - 15% test
   - Stratified split Ä‘á»ƒ giá»¯ class balance

4. **Class imbalance handling**
   - TÃ­nh class weight ratio
   - LÆ°u metadata vá» class distribution
   - Document strategy xá»­ lÃ½ imbalance (SMOTE? Weighted loss?)

5. **Save processed data**
   - Parquet format cho train/valid/test
   - JSON metadata file (counts, distributions, feature names)

#### Output:
- Script `cardiac_data_prep_v2.py` hoÃ n chá»‰nh
- 3 parquet files: train_data, valid_data, test_data
- metadata.json vá»›i thÃ´ng tin dataset

---

### BÆ¯á»šC 1.5: Update Producer Schema (3 giá»)

#### Nhiá»‡m vá»¥ chÃ­nh:

1. **Analyze schema changes**
   - So sÃ¡nh schema cÅ© vs schema má»›i
   - Identify breaking changes
   - Plan migration strategy

2. **Update Kafka message schema**
   - Modify `cardiac_producer_v2.py`
   - Update field names Ä‘á»ƒ match dataset má»›i
   - Update data types
   - Update value ranges

3. **Test message generation**
   - Generate sample messages vá»›i schema má»›i
   - Verify JSON serialization
   - Check message size
   - Test Kafka publishing

4. **Update schema documentation**
   - Document má»—i field trong message
   - Provide example messages
   - Medical context cho tá»«ng field

#### Output:
- `cardiac_producer_v2.py` vá»›i schema má»›i
- Schema documentation (markdown)
- Test script verify message generation

---

### BÆ¯á»šC 1.6: Update Streaming Inference Schema (3 giá»)

#### Nhiá»‡m vá»¥ chÃ­nh:

1. **Update Spark StructType**
   - File: `cardiac_streaming_inference_v2.py`
   - Match vá»›i schema má»›i tá»« producer
   - Ensure data types compatible

2. **Update feature vector assembly**
   - List continuous features má»›i
   - List binary features má»›i
   - Update VectorAssembler configuration

3. **Test streaming pipeline**
   - Run producer vá»›i data má»›i
   - Consume tá»« Kafka
   - Verify schema parsing
   - Check for null values

4. **Handle schema evolution**
   - Strategy khi schema thay Ä‘á»•i trong tÆ°Æ¡ng lai
   - Backward compatibility considerations

#### Output:
- `cardiac_streaming_inference_v2.py` updated
- Integration test passed (producer â†’ kafka â†’ spark)
- Documentation vá» schema evolution strategy

---

### BÆ¯á»šC 1.7: End-to-End Data Pipeline Test (2 giá»)

#### Nhiá»‡m vá»¥ chÃ­nh:

1. **Run complete pipeline**
   - Execute data prep script
   - Verify train/valid/test files created
   - Start producer vá»›i schema má»›i
   - Start streaming inference
   - Check PostgreSQL cho predictions

2. **Validation checks**
   - Row counts match expected
   - No data corruption
   - Feature distributions reasonable
   - No null values in critical fields

3. **Performance benchmarking**
   - Data prep execution time
   - Streaming throughput (messages/second)
   - Inference latency

#### Output:
- Test report document
- Performance metrics baseline
- Checklist: âœ… Phase 1 complete
